# model_path = "../models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf"
# template = "llama3"
# model_path = "../models/internlm2_5-7b-chat-Q5_K_M.gguf"
# template = "qwen"
# prompts = "./static/prompt.tool.toml"
model_path = "../models/causallm_7b.Q6_K.gguf"
template = "qwen"
prompts = "./static/prompt.map.toml"

[run]
ctx_size = 4096
n_batch = 64
n_gpu_layers = 100

[templates.qwen]
header_prefix = "<|im_start|>"
header_suffix = "\n"
end_of_content = "<|im_end|>\n"
stops = ["<|im_end|>", "<|im_end|>\n", "<|im_"]

[templates.llama3]
header_prefix = "<|start_header_id|>"
header_suffix = "<|end_header_id|>\n"
end_of_content = "<|eot_id|>\n"
stops = ["<|eot_id|>"]

[templates.gemma2]
header_prefix = "<|start_of_turn|>"
header_suffix = "\n"
end_of_content = "<|end_of_turn|>\n"
stops = ["<|end_of_turn|>"]

[templates.phi-3]
header_prefix = "<|"
header_suffix = "|>\n"
end_of_content = "<|end|>\n"
stops = ["<|end|>"]
